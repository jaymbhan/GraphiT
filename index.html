<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>6.796 Project — Encoding Graph Structure in Transformers</title>

  <!-- Simple modern reset + layout -->
  <style>
    :root{
      --bg: #ffffff;
      --muted: #6b7280;
      --accent: #0f172a;
      --primary: #0ea5a4;
      --paper-width: 880px;
      --sidebar-width: 260px;
      --max-content-width: 820px;
      --serif: "Source Serif Pro", Georgia, "Times New Roman", serif;
      --sans: Inter, ui-sans-serif, system-ui, -apple-system, "Segoe UI", Roboto, "Helvetica Neue", Arial;
    }
    @import url('https://fonts.googleapis.com/css2?family=Source+Serif+4:wght@400;600;700&family=Inter:wght@300;400;600;700&display=swap');

    html,body{
      height:100%;
      margin:0;
      font-family:var(--sans);
      color:#0b1220;
      background: linear-gradient(180deg,#fbfcfd 0%, #f7fafc 100%);
      -webkit-font-smoothing:antialiased;
      -moz-osx-font-smoothing:grayscale;
    }

    .container{
      max-width: calc(var(--paper-width) + var(--sidebar-width) + 80px);
      margin: 36px auto;
      display: grid;
      grid-template-columns: var(--sidebar-width) 1fr;
      gap: 28px;
      padding: 0 20px;
    }

    /* Sidebar / TOC */
    .toc {
      position: sticky;
      top: 28px;
      align-self:start;
      background: #fff;
      border-radius: 10px;
      padding: 18px;
      box-shadow: 0 4px 18px rgba(12,20,30,0.06);
      border: 1px solid rgba(10,12,20,0.04);
      height: fit-content;
      font-size: 14px;
    }
    .toc h3 {
      font-family: var(--serif);
      margin: 0 0 10px 0;
      font-weight:600;
      color: var(--accent);
    }
    .toc ul { padding-left: 14px; margin: 6px 0 0 0; }
    .toc a {
      color: #0b1220;
      text-decoration: none;
      display:block;
      padding:6px 0;
      color: var(--muted);
    }
    .toc a:hover { color: var(--primary); }

    /* Content */
    .article {
      background: #fff;
      border-radius: 12px;
      padding: 34px;
      box-shadow: 0 8px 36px rgba(12,20,30,0.06);
      border: 1px solid rgba(10,12,20,0.04);
      max-width: var(--max-content-width);
    }
    header .title {
      font-family: var(--serif);
      font-size: 34px;
      margin: 0 0 6px 0;
      color: var(--accent);
    }
    header .authors {
      color: var(--muted);
      font-size: 14px;
      margin-bottom: 18px;
    }
    .lead {
      color: #334155;
      font-size: 16px;
      line-height:1.6;
      margin-bottom: 20px;
    }

    h2 {
      font-family: var(--serif);
      color: #071029;
      margin-top:28px;
      margin-bottom:10px;
      font-size:22px;
    }
    h3 {
      font-family: var(--serif);
      margin-top:18px;
      margin-bottom:8px;
      color:#0b1220;
    }
    p { color:#334155; line-height:1.7; font-size:15px; margin: 10px 0; }

    /* Figure */
    .figure {
      margin: 16px 0;
      text-align:center;
    }
    .figure img {
      max-width:100%;
      height:auto;
      border-radius:8px;
      box-shadow: 0 6px 18px rgba(15,23,42,0.06);
      border:1px solid rgba(10,12,20,0.04);
    }
    .figcap {
      font-size:13px;
      color:var(--muted);
      margin-top:8px;
      font-style:italic;
    }

    /* Table styling */
    table {
      width:100%;
      border-collapse: collapse;
      margin: 12px 0 20px 0;
      font-size:14px;
    }
    thead th {
      text-align:left;
      padding:10px 12px;
      background: linear-gradient(90deg,#f8fafc,#ffffff);
      border-bottom:1px solid rgba(11,17,29,0.06);
      color: #0b1220;
      font-weight:600;
    }
    tbody td {
      padding:10px 12px;
      border-bottom:1px dashed rgba(11,17,29,0.06);
      color:#253143;
    }
    tbody tr:last-child td { border-bottom: none; }

    .best { font-weight:700; color:var(--primary); }
    .second { font-style:italic; color:#0b1220; }

    /* Code styling (for inline formulas or snippets) */
    pre, code {
      font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, "Roboto Mono", monospace;
      background:#0f172a0a;
      padding:6px 8px;
      border-radius:6px;
      font-size:13px;
    }

    /* Responsiveness */
    @media (max-width: 980px){
      .container { grid-template-columns: 1fr; padding: 12px; }
      .toc { position: relative; top:auto; margin-bottom:12px; }
    }
    footer {
      margin-top: 22px;
      color:var(--muted);
      font-size:13px;
      border-top:1px solid rgba(10,12,20,0.04);
      padding-top:12px;
    }
    .math {
      background:linear-gradient(90deg,#ffffff,#fbfeff);
      padding:10px;
      border-radius:8px;
      border:1px solid rgba(10,12,20,0.03);
      margin:12px 0;
    }
  </style>

  <!-- MathJax -->
  <script>
    window.MathJax = {
      tex: { inlineMath: [['\\(','\\)'], ['$', '$']], displayMath: [['\\[','\\]'], ['$$','$$']] },
      svg: { fontCache: 'global' }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" defer></script>
</head>
<body>
  <div class="container">
    <!-- SIDEBAR / TOC -->
    <nav class="toc" aria-label="Table of contents">
      <h3>Contents</h3>
      <ul>
        <li><a href="#introduction">Introduction</a></li>
        <li><a href="#prior-work">Prior Work</a></li>
        <li><a href="#methods">Methods & Experiments</a>
          <ul>
            <li><a href="#problems">3.1 Problems</a></li>
            <li><a href="#dataset">3.2 Dataset generation</a></li>
            <li><a href="#posenc">3.3 Positional encodings</a></li>
            <li><a href="#experiment">3.4 Experiment</a></li>
          </ul>
        </li>
        <li><a href="#results">Results (Table)</a></li>
        <li><a href="#discussion">Discussion</a></li>
        <li><a href="#conclusion">Conclusion</a></li>
        <li><a href="#references">References</a></li>
      </ul>
    </nav>

    <!-- ARTICLE -->
    <article class="article">
      <header>
        <div class="title">Encoding Graph Structure in Transformers: Positional Kernels & Tasks</div>
        <div class="authors">Jay Bhan, Allen Lin, and Fiona Lu — 6.796 Project</div>
      </header>

      <p class="lead">
        We study how different positional encoding kernels affect graph transformers' ability to solve structural graph problems.
        We test a diverse set of kernels — local, spectral, and walk-based — across three canonical problems: <strong>CLIQUE</strong>, <strong>BIPARTITE</strong>, and <strong>CYCLE</strong>.
      </p>

      <h2 id="introduction">1. Introduction</h2>
      <p>
        Graphs provide a fundamental framework for modeling pairwise relations across many domains. While graph neural networks (GNNs) excel at local aggregation, <em>graph transformers</em> capture long-range dependencies but require effective positional encodings to represent graph topology. Following recent kernel-based approaches, we test a set of positional kernels and analyze their effect on task performance.
      </p>

      <h2 id="prior-work">2. Prior Work</h2>
      <p>
        Many GNN positional encodings stem from spectral theory: represent the Laplacian \(L = D - A\) by its eigendecomposition
      </p>
      <div class="math">
        \[
          L = \sum_{i=1}^n \lambda_i v_i v_i^\top,
          \qquad
          K_r = \sum_{i=1}^n r(\lambda_i) v_i v_i^\top.
        \]
      </div>
      <p>
        Different choices of the regularizer \(r(\lambda)\) yield diffusion kernels, random-walk kernels, Estrada kernels, cosine-based spectral transforms, and more.
      </p>

      <h2 id="methods">3. Methods & Experiments</h2>

      <h3 id="problems">3.1 Problems</h3>
      <p><strong>CLIQUE.</strong> Predict the size of the largest clique (local property).</p>
      <p><strong>BIPARTITE.</strong> Decide whether a graph is bipartite/tripartite (global parity property).</p>
      <p><strong>CYCLE.</strong> Predict the size of the largest cycle (mid-to-long-range structural property).</p>

      <div class="figure">
        <img src="./images/figure1.png" alt="Figure 1: Example graph (placeholder)">
        <div class="figcap">Figure 1: A graph with 5 nodes (placeholder). Replace <code>images/figure1.png</code> with the actual figure.</div>
      </div>

      <h3 id="dataset">3.2 Dataset generation</h3>
      <p>
        Datasets were synthesized using NetworkX. CLIQUE graphs: 20-node Erdős–Rényi draws at \(p=0.5\), sampled until balanced clique-size classes were obtained. BIPARTITE: generated via forced partitioning. CYCLE: sparse graphs with low edge probability to produce a variety of cycle sizes. Full details and random seeds are in the repository.
      </p>

      <div class="figure">
        <img src="./images/figure2.png" alt="Figure 2: Bipartite example (placeholder)">
        <div class="figcap">Figure 2: Bipartite example (placeholder). Replace <code>images/figure2.png</code> when available.</div>
      </div>

      <h3 id="posenc">3.3 Positional encodings</h3>
      <p>
        We tested the following kernels (implemented as \(n\times n\) kernels or diagonal node features):
      </p>
      <ul>
        <li><strong>Diffusion:</strong> \(r(\lambda)=e^{-\beta\lambda}\), \(K=e^{-\beta L}\).</li>
        <li><strong>p-step random walk:</strong> \(r(\lambda)=(1-\gamma\lambda)^p\), \(K=(I-\gamma L)^p\).</li>
        <li><strong>Adjacency:</strong> use \(A\) directly as a relative positional kernel.</li>
        <li><strong>Shortest-path:</strong> \(K_{ij}=1/\text{dist}(i,j)\) with diagonal set to 1.</li>
        <li><strong>Graphlet:</strong> \(K = V V^\top\) where \(V\) contains local motif counts per node.</li>
        <li><strong>Coloring:</strong> kernel indicating nodes sharing the same greedy color.</li>
        <li><strong>Estrada:</strong> node subgraph centrality from \((e^A)_{ii}\) or full \(e^{A}\) kernel.</li>
        <li><strong>Cosine (spectral):</strong> \(r(\lambda)=\cos(\tfrac{\pi}{4}\lambda)\), implemented as \(\cos(\tfrac{\pi}{4}L)\) after eigenvalue normalization.</li>
        <li><strong>Clustering coefficient:</strong> diagonal kernel with local clustering per node.</li>
        <li><strong>Betweenness centrality:</strong> diagonal kernel with node betweenness.</li>
        <li><strong>None:</strong> no positional encoding.</li>
      </ul>

      <h3 id="experiment">3.4 Experiment</h3>
      <p>
        For each kernel and each problem we trained a Graph Transformer for 300 epochs, keeping architecture and hyperparameters fixed. Test accuracies are reported in Table 1.
      </p>

      <h2 id="results">Results</h2>
      <p>
        Table 1 lists test accuracies (higher is better). For each task the best and second-best results are highlighted.
      </p>

      <!-- Accuracy table -->
      <table aria-label="Accuracy of positional embedding kernels">
        <thead>
          <tr>
            <th>Positional encoding</th>
            <th>CLIQUE</th>
            <th>BIPARTITE</th>
            <th>CYCLE</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>Diffusion</td><td>0.6491</td><td>0.9373</td><td>0.7469</td></tr>
          <tr><td>P-step</td><td>0.6717</td><td class="second">0.9975</td><td>0.1529</td></tr>
          <tr><td class="best">Adjacency</td><td class="best">0.6967</td><td>0.9825</td><td>0.6667</td></tr>
          <tr><td>Shortest Path</td><td>0.6692</td><td>0.9323</td><td class="best">0.8647</td></tr>
          <tr><td>Graphlet</td><td>0.6040</td><td>0.9123</td><td>0.4236</td></tr>
          <tr><td>Coloring</td><td>0.6717</td><td>0.9549</td><td>0.4236</td></tr>
          <tr><td>Estrada</td><td>0.6391</td><td>0.8246</td><td>0.3759</td></tr>
          <tr><td>Cosine</td><td>0.2657</td><td>0.8045</td><td>0.2907</td></tr>
          <tr><td>Clustering Coefficient</td><td>0.6466</td><td class="best">1.0000</td><td>0.3283</td></tr>
          <tr><td>Betweenness Centrality</td><td>0.6241</td><td>0.8596</td><td>FAILED</td></tr>
          <tr><td>None</td><td>0.6316</td><td>0.8396</td><td>0.2206</td></tr>
        </tbody>
      </table>

      <p class="figcap" style="margin-top:-8px;">Table 1. Test accuracies for positional embedding kernels across problems. <strong>Best</strong> shown in bold; <em>second-best</em> italicized.</p>

      <h2 id="discussion">4. Discussion</h2>
      <p>
        The experimental patterns align with the mathematical properties of the kernels:
      </p>
      <ul>
        <li><strong>Adjacency (local)</strong> is best for CLIQUE (0.6967) — consistent with clique detection being a local connectivity task.</li>
        <li><strong>Bipartite</strong> is comparatively easy: many kernels (clustering coefficient, p-step, adjacency, coloring) get ≈0.95–1.00, showing the property leaves strong spectral and local signatures.</li>
        <li><strong>Cycle length</strong> requires distance-aware information; shortest-path wins (0.8647) while many smoothing or motif-based kernels underperform.</li>
      </ul>

      <div class="math">
        A key spectral viewpoint: a kernel built from eigenpairs of \(L\),
        \[
          K_r = \sum_{i} r(\lambda_i) v_i v_i^\top,
        \]
        biases the model toward eigenvectors with large weights \(r(\lambda)\). Local kernels preserve high-frequency (localized) components; diffusion-type kernels emphasize low-frequency smooth structure.
      </div>

      <h2 id="conclusion">5. Conclusion</h2>
      <p>
        No single kernel dominates across all problems — positional encodings must be matched to the structural demands of the task. Local adjacency excels for local clique detection, many kernels suffice to detect bipartiteness, and explicit distance encodings best capture cycle geometry. Future work could explore hybrid or learnable kernels that adaptively combine local, spectral, and distance information.
      </p>

      <div class="figure">
        <img src="./images/figure3.png" alt="Figure 3: Example cycle graph (placeholder)">
        <div class="figcap">Figure 3: Example cycle illustration (placeholder). Replace <code>images/figure3.png</code> with your figure file.</div>
      </div>

      <h2 id="references">References</h2>
      <ol>
        <li>Vaswani et al., "Attention is All You Need", NeurIPS 2017.</li>
        <li>Dosovitskiy et al., "ViT", ICLR 2021.</li>
        <li>Mialon et al., "GraphiT: Encoding Graph Structure in Transformers", arXiv 2021.</li>
        <li>Estrada, "Characterization of 3D molecular structure", Chemical Physics Letters, 2000.</li>
        <li>... (add the rest of your references as needed)</li>
      </ol>

      <footer>
        <p>Project: Jay Bhan, Allen Lin, Fiona Lu — Massachusetts Institute of Technology. Source PDF adapted into this web page.</p>
      </footer>

    </article>
  </div>

  <!-- Optional: small helper script that highlights TOC active link (simple) -->
  <script>
    // Simple scrollspy to highlight TOC (works on small pages)
    (function(){
      const links = document.querySelectorAll('.toc a');
      const sections = Array.from(links).map(a => document.querySelector(a.getAttribute('href')));
      function onScroll(){
        const top = window.scrollY + 120;
        let idx = sections.findIndex(s => s && (s.offsetTop + s.offsetHeight) > top);
        if (idx === -1) idx = sections.length-1;
        links.forEach((l,i)=> l.style.fontWeight = (i===idx ? '600' : '400'));
      }
      window.addEventListener('scroll', onScroll);
      setTimeout(onScroll,500);
    })();
  </script>
</body>
</html>